{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blind-outreach",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Thinking of making batch size large and just use the batch as the set of images to cluster\n",
    "- If want to go for variable dataset sizes, can just have an outer loop that will change the batch size but train the same model\n",
    "\n",
    "## Comparison\n",
    "- Amortized\n",
    "    - PermNet: 23.5 sec / iter\n",
    "- Not Amortized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-remainder",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cross-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointnet import PointNet\n",
    "from permnet import PermNet\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import *\n",
    "from pyro.infer import Predictive, SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, AdamW\n",
    "from pyro.nn import PyroModule\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-approval",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polish-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f\n",
    "\n",
    "def mnist_data():\n",
    "    compose = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((.5), (.5))\n",
    "        ])\n",
    "    out_dir = './data/mnist'\n",
    "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
    "\n",
    "def images_to_vectors(images):\n",
    "    return images.view(images.size(0), 784)\n",
    "\n",
    "def vectors_to_images(vectors):\n",
    "    return vectors.view(vectors.size(0), 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "toxic-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "N = 100\n",
    "data = mnist_data()\n",
    "# Create loader with data, so that we can iterate over it\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=N, shuffle=True)\n",
    "# Num batches\n",
    "num_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-battery",
   "metadata": {},
   "source": [
    "### Pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "final-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, step, assignment_list):\n",
    "    # global variables\n",
    "    alpha = torch.ones(T)\n",
    "    weights = pyro.sample('weights', Dirichlet(alpha))\n",
    "    \n",
    "    with pyro.plate('components', T):\n",
    "        locs = pyro.sample('locs', MultivariateNormal(torch.zeros(M), torch.eye(M)))\n",
    "\n",
    "    # use PCA to reduce dimensionality so I can train faster\n",
    "    img_set = images_to_vectors(data.squeeze())\n",
    "    (U, S, V) = torch.pca_lowrank(img_set, q=None, center=True, niter=10)\n",
    "    z = torch.matmul(data, V[:, :100000])\n",
    "    # local variables\n",
    "    with pyro.plate('data', N):\n",
    "        assignment = pyro.sample('assignments', Categorical(weights))\n",
    "        pyro.sample('obs', MultivariateNormal(locs[assignment], torch.eye(M)), obs=z)\n",
    "        \n",
    "def guide(data, step, assignment_list):\n",
    "    # train nn if doing offline training\n",
    "    if not amortize:\n",
    "        pyro.module('alpha_mlp', alpha_mlp)\n",
    "        pyro.module('tau_mlp', tau_mlp)\n",
    "    \n",
    "    if use_gpu: \n",
    "        data = data.cuda()\n",
    "        \n",
    "    tau = tau_mlp(data.float())\n",
    "    tau = tau.view(T,M)\n",
    "\n",
    "    # sample mixture components mu\n",
    "    with pyro.plate('components', T):\n",
    "        locs = pyro.sample('locs', MultivariateNormal(tau, torch.eye(M)))\n",
    "    \n",
    "    # sample cluster assignments\n",
    "    alpha = alpha_mlp(data.float()) # returns a vector of length T\n",
    "    weights = pyro.sample('weights', Dirichlet(alpha))  # vector of length T\n",
    "    with pyro.plate('data', size=N):\n",
    "        assignments = pyro.sample('assignments', Categorical(weights))\n",
    "    \n",
    "    # logging\n",
    "    if step % log_iter == 0:\n",
    "        \n",
    "        print('='*10, 'Iteration {}'.format(step), '='*10)\n",
    "        assignment_list.append(assignments)\n",
    "        \n",
    "        weight_data = [weights[0][i] for i in range(len(weights[0]))]\n",
    "        weight_data.insert(0, 'props')\n",
    "\n",
    "        pc1_data = [locs[i,0] for i in range(locs.shape[0])]\n",
    "        pc1_data.insert(0, 'pc1')\n",
    "\n",
    "        pc2_data = [locs[i,1] for i in range(locs.shape[0])]\n",
    "        pc2_data.insert(0, 'pc2')\n",
    "        \n",
    "        pc3_data = [locs[i,2] for i in range(locs.shape[0])]\n",
    "        pc3_data.insert(0, 'pc3')\n",
    "        \n",
    "        pc4_data = [locs[i,3] for i in range(locs.shape[0])]\n",
    "        pc4_data.insert(0, 'pc4')\n",
    "        \n",
    "        pc5_data = [locs[i,4] for i in range(locs.shape[0])]\n",
    "        pc5_data.insert(0, 'pc5')\n",
    "        \n",
    "        pc6_data = [locs[i,5] for i in range(locs.shape[0])]\n",
    "        pc6_data.insert(0, 'pc6')\n",
    "        \n",
    "        \n",
    "        data = [weight_data, pc1_data, pc2_data, pc3_data, pc4_data, pc5_data, pc6_data]\n",
    "        \n",
    "        print(tabulate(data, headers=['', 'clust1', 'clust2', 'clust3', 'clust4', 'clust5', 'clust6', 'clust7', 'clust8', 'clust9', 'clust10']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-scene",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "failing-museum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not using GPU!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyang/.local/lib/python3.7/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "T = 10  # num components\n",
    "M = 6  # how many features after pca\n",
    "\n",
    "mlp_type = 'permnet'\n",
    "amortize = False\n",
    "\n",
    "if mlp_type == 'pointnet':\n",
    "    alpha_mlp = PointNet(in_ch=784, output_size=T).float() \n",
    "    tau_mlp = PointNet(in_ch=784, output_size=T*M, softmax=False).float()\n",
    "if mlp_type == 'permnet':\n",
    "    alpha_mlp = PermNet(in_ch=784, hidden=2048, output_size=T).float()\n",
    "    tau_mlp = PermNet(in_ch=784, hidden=2048, output_size=T*M).float()\n",
    "    \n",
    "if amortize:\n",
    "    saved_alpha_mlp = torch.load('saved_models/{}/mnist_alpha_mlp.pth'.format(mlp_type))\n",
    "    saved_tau_mlp = torch.load('saved_models/{}/mnist_tau_mlp.pth'.format(mlp_type))\n",
    "    \n",
    "    alpha_mlp.load_state_dict(saved_alpha_mlp['model_state_dict'])\n",
    "    tau_mlp.load_state_dict(saved_tau_mlp['model_state_dict'])\n",
    "\n",
    "adam_params = {\"lr\": 0.001}\n",
    "optimizer = Adam(adam_params)\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('using GPU!')\n",
    "    alpha_mlp = alpha_mlp.cuda()\n",
    "    tau_mlp = tau_mlp.cuda()\n",
    "else:\n",
    "    print('not using GPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-backing",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "smooth-wedding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Iteration 0 ==========\n",
      "          clust1    clust2    clust3    clust4     clust5    clust6    clust7     clust8     clust9    clust10\n",
      "-----  ---------  --------  --------  --------  ---------  --------  --------  ---------  ---------  ---------\n",
      "props  0.0560008  0.081129  0.128321  0.152375  0.0868095  0.129879  0.199609  0.0231701  0.0213998   0.121306\n",
      "pc1    2.52577    3.57674   1.66435   3.25925   3.72857    2.83758   1.97311   2.81527    1.83997     2.00483\n",
      "pc2    1.95136    0.810007  1.29679   1.73987   1.82048    3.40796   2.23508   1.16216    0.943383    1.74741\n",
      "pc3    1.37477    2.3139    2.43395   1.11174   1.6144     0.224904  0.620918  3.94495    0.899404    0.551945\n",
      "pc4    2.04916    2.1129    2.84624   1.20164   1.21152    2.03075   2.37417   2.1499     3.08445     0.531404\n",
      "pc5    1.86716    1.52568   1.18449   1.1363    0.467865   2.43348   0.100659  1.35229    3.08058     5.30868\n",
      "pc6    0.66599    1.67351   3.10608   0.73849   1.8671     2.13287   2.33482   3.10941    1.52882     0.140203\n",
      "took 12.543780326843262 seconds\n",
      "========== Iteration 2 ==========\n",
      "         clust1     clust2     clust3     clust4     clust5      clust6     clust7      clust8     clust9    clust10\n",
      "-----  --------  ---------  ---------  ---------  ---------  ----------  ---------  ----------  ---------  ---------\n",
      "props  0.158825  0.0598019  0.0615178   0.128585  0.0433511   0.0107293   0.120973  0.00854471  0.0383847   0.369287\n",
      "pc1    4.77104   4.32966    6.94948    12.9063    9.97634     6.69693    11.7786    2.62331     2.75792     7.82678\n",
      "pc2    3.59972   2.74558    5.78299    12.7153    7.75125    11.6543     11.4107    3.7172      3.07528    10.3279\n",
      "pc3    1.70834   5.00503    7.99777     5.75843   3.85122     5.39625     4.98044   1.88617     2.47262     2.67123\n",
      "pc4    3.71063   2.93798    4.97851     5.07688   2.10623     4.62787     6.32725   2.05435     1.63824     1.69567\n",
      "pc5    2.34839   3.44438    4.96507     8.13198   1.56946     2.62638     5.87192   2.06385     2.87077     4.21693\n",
      "pc6    2.74251   1.07491    2.27717     1.46831   3.64086     2.27279     2.02211   2.22171     1.63497     2.48207\n",
      "took 25.41887903213501 seconds\n",
      "========== Iteration 4 ==========\n",
      "          clust1     clust2     clust3     clust4    clust5     clust6      clust7      clust8     clust9    clust10\n",
      "-----  ---------  ---------  ---------  ---------  --------  ---------  ----------  ----------  ---------  ---------\n",
      "props  0.0883832  0.0993343  0.0839027   0.151941  0.102875  0.0676757   0.0361404   0.0758872  0.0696422   0.224219\n",
      "pc1    5.16144    2.7532     3.95366    12.2482    6.34771   4.6385     13.7425      0.434861   0.347348    8.5\n",
      "pc2    3.84854    2.11507    6.17651     7.52434   6.0792    7.96003     8.43838     0.9743     2.73574     8.66088\n",
      "pc3    3.05999    2.47043    7.04091     3.20233   3.55206   2.49247     7.63622    -0.222026   2.08752     4.58982\n",
      "pc4    0.82382    1.95915    2.00446     2.95496   0.798932  3.33915     3.43388    -1.32554    1.22141     1.74021\n",
      "pc5    0.0257194  2.73681    4.00204     3.99505   2.29318   2.18654     4.08401    -0.0864027  3.66208     5.82951\n",
      "pc6    1.44884    2.07067    1.31007     0.638212  0.378492  0.533689    2.51999     1.50991    0.58653     2.22718\n",
      "took 26.874974012374878 seconds\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "dry = False\n",
    "log_iter = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for n_batch, (real_batch,_) in enumerate(data_loader):\n",
    "        \n",
    "        data = images_to_vectors(real_batch).unsqueeze(0)\n",
    "        \n",
    "        assignment_list = []\n",
    "        n_steps = 5\n",
    "        start = time.time()\n",
    "        for step in range(n_steps):\n",
    "            svi.step(data, step, assignment_list)\n",
    "            pyro.get_param_store()\n",
    "            if step % log_iter == 0:\n",
    "                end = time.time()\n",
    "                print('took', end-start, 'seconds')\n",
    "                start = time.time()\n",
    "\n",
    "        if not dry:\n",
    "            torch.save({'model_state_dict': alpha_mlp.state_dict(),\n",
    "                        'assignments': assignment_list[-1],\n",
    "                        'steps': n_steps\n",
    "                       }, 'saved_models/{}/mnist_alpha_mlp.pth'.format(mlp_type))\n",
    "\n",
    "            torch.save({'model_state_dict': tau_mlp.state_dict(),\n",
    "                        'assignments': assignment_list[-1],\n",
    "                        'steps': n_steps\n",
    "                       }, 'saved_models/{}/mnist_tau_mlp.pth'.format(mlp_type))\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc533y",
   "language": "python",
   "name": "cpsc533y"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

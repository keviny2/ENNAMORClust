{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heavy-attitude",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notes\n",
    "\n",
    "The equivariant neural network implementation is loosely based on [PointNet](https://arxiv.org/pdf/1612.00593.pdf). I only implemented the first 'standard transform layer', but could design a much more detailed one. Here is the [GitHub repo](https://github.com/fxia22/pointnet.pytorch/blob/f0c2430b0b1529e3f76fb5d6cd6ca14be763d975/pointnet/model.py#L11) to the code\n",
    "\n",
    "## Hyperparameters\n",
    "- Adam{\"lr\": 0.00001} / SELU() / max / SVI\n",
    "\n",
    "## Comparison\n",
    "- Amortized\n",
    "    - PermNet: 14 sec / iter\n",
    "- Not Amortized\n",
    "    - PermNet: 15 sec / iter\n",
    "\n",
    "## BUGS\n",
    "- cluster degeneracy w/ pointnet (one cluster makes up the majority of the proportion vector)\n",
    "    - [DEEP UNSUPERVISED CLUSTERING WITH GAUSSIAN MIXTURE VARIATIONAL AUTOENCODERS](https://arxiv.org/pdf/1611.02648.pdf)\n",
    "\n",
    "\n",
    "## Resources\n",
    "- [Tutorial on to_event and .expand()](https://bochang.me/blog/posts/pytorch-distributions/)\n",
    "- [Event, Batch, Sample shapes](https://ericmjl.github.io/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/)\n",
    "- [Debugging Neural Networks](https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-acquisition",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointnet import PointNet\n",
    "from permnet import PermNet\n",
    "from convnet import ConvNet\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import *\n",
    "from pyro.infer import Predictive, SVI, Trace_ELBO\n",
    "from pyro.optim import Adam, AdamW\n",
    "from pyro.nn import PyroModule\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-boundary",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500  # number of data points\n",
    "M = 2  # number of features\n",
    "T = 5  # Fixed number of components.\n",
    "\n",
    "cov = np.identity(M)  # covariance matrix is just the identity for now\n",
    "\n",
    "# generate data\n",
    "clust1 = np.random.multivariate_normal(np.zeros(M), cov, 100)\n",
    "clust2 = np.random.multivariate_normal(np.ones(M)*10, cov, 100)\n",
    "clust3 = np.random.multivariate_normal(np.ones(M)*-10, cov, 100)\n",
    "clust4 = np.random.multivariate_normal([10, -10], cov, 100)\n",
    "clust5 = np.random.multivariate_normal([-10, 10], cov, 100)\n",
    "data = np.concatenate((clust1, clust2, clust3, clust4, clust5))\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "plt.grid()\n",
    "\n",
    "data = torch.from_numpy(data).float()  # convert numpy to torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-washington",
   "metadata": {},
   "source": [
    "### Pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, step, assignment_list):\n",
    "    # global variables\n",
    "    alpha = torch.ones(T)\n",
    "    weights = pyro.sample('weights', Dirichlet(alpha))\n",
    "    \n",
    "    with pyro.plate('components', T):\n",
    "        locs = pyro.sample('locs', MultivariateNormal(torch.zeros(M), torch.eye(M)))\n",
    "\n",
    "    # local variables\n",
    "    with pyro.plate('data', N):\n",
    "        assignment = pyro.sample('assignments', Categorical(weights))\n",
    "        pyro.sample('obs', MultivariateNormal(locs[assignment], torch.eye(M)), obs=data)\n",
    "        \n",
    "def guide(data, step, assignment_list):\n",
    "    \n",
    "    # train nn if doing offline training\n",
    "    if not amortize:\n",
    "        pyro.module('alpha_mlp', alpha_mlp)\n",
    "        pyro.module('tau_mlp', tau_mlp)\n",
    "    \n",
    "    if use_gpu: \n",
    "        data = data.cuda()\n",
    "    \n",
    "    # sample mixture components mu\n",
    "    tau = tau_mlp(data.float())\n",
    "    tau = tau.view(T,M)  # reshape tensor\n",
    "    \n",
    "    with pyro.plate('components', T):\n",
    "        locs = pyro.sample('locs', MultivariateNormal(tau, torch.eye(M)))\n",
    "    \n",
    "    # sample cluster assignments\n",
    "    alpha = alpha_mlp(data.float()) # returns a vector of length T\n",
    "    weights = pyro.sample('weights', Dirichlet(alpha))  # vector of length T\n",
    "    with pyro.plate('data', N):\n",
    "        assignments = pyro.sample('assignments', Categorical(weights))\n",
    "    \n",
    "    if step % log_iter == 0:\n",
    "        print('='*10, 'Iteration {}'.format(step), '='*10)\n",
    "        \n",
    "        assignment_list.append(assignments)\n",
    "        \n",
    "        weight_data = [weights.squeeze()[i] for i in range(len(weights.squeeze()))]\n",
    "        weight_data.insert(0, 'props')\n",
    "\n",
    "        mu1_data = [locs[i,0] for i in range(locs.shape[0])]\n",
    "        mu1_data.insert(0, 'mu1')\n",
    "\n",
    "        mu2_data = [locs[i,1] for i in range(locs.shape[0])]\n",
    "        mu2_data.insert(0, 'mu2')\n",
    "        \n",
    "        data = [weight_data, mu1_data, mu2_data]\n",
    "        \n",
    "        print(tabulate(data, headers=['', 'clust1', 'clust2', 'clust3', 'clust4', 'clust5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-berlin",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_type = 'permnet'\n",
    "amortize = False\n",
    "\n",
    "\n",
    "if mlp_type == 'pointnet':\n",
    "    alpha_mlp = PointNet(in_ch=M, output_size=T).float() \n",
    "    tau_mlp = PointNet(in_ch=M, output_size=T*M, softmax=False).float()\n",
    "if mlp_type == 'permnet':\n",
    "    alpha_mlp = PermNet(in_ch=M, hidden=256, output_size=T).float()\n",
    "    tau_mlp = PermNet(in_ch=M, hidden=256, output_size=T*M).float()\n",
    "if mlp_type == 'convnet':\n",
    "    alpha_mlp = ConvNet(in_ch=M, hidden=256, output_size=T).float()\n",
    "    tau_mlp = ConvNet(in_ch=M, hidden=256, output_size=T*M).float()\n",
    "    \n",
    "if amortize:\n",
    "    saved_alpha_mlp = torch.load('saved_models/{}/vanilla_alpha_mlp.pth'.format(mlp_type))\n",
    "    saved_tau_mlp = torch.load('saved_models/{}/vanilla_tau_mlp.pth'.format(mlp_type))\n",
    "    \n",
    "    alpha_mlp.load_state_dict(saved_alpha_mlp['model_state_dict'])\n",
    "    tau_mlp.load_state_dict(saved_tau_mlp['model_state_dict'])\n",
    "    \n",
    "\n",
    "adam_params = {\"lr\": 0.001}\n",
    "optimizer = Adam(adam_params)\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "# use gpu\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('using GPU!')\n",
    "    alpha_mlp = alpha_mlp.cuda()\n",
    "    tau_mlp = tau_mlp.cuda()\n",
    "else:\n",
    "    print('not using GPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-track",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry = True\n",
    "log_iter = 100\n",
    "\n",
    "n_steps = 500\n",
    "start = time.time()\n",
    "\n",
    "assignment_list = []\n",
    "elbo_ests = []\n",
    "for step in range(n_steps):\n",
    "    elbo_est = svi.step(data, step, assignment_list)\n",
    "    elbo_ests.append(elbo_est)\n",
    "    if step % log_iter == 0:\n",
    "        end = time.time()\n",
    "        print('took', end-start, 'seconds')\n",
    "        start = time.time()\n",
    "        \n",
    "    if not dry:\n",
    "        torch.save({'model_state_dict': alpha_mlp.state_dict(),\n",
    "                    'assignments': assignment_list[-1],\n",
    "                    'elbo_ests': elbo_ests,\n",
    "                    'steps': n_steps\n",
    "                   }, 'saved_models/{}/mnist_alpha_mlp.pth'.format(mlp_type))\n",
    "\n",
    "        torch.save({'model_state_dict': tau_mlp.state_dict(),\n",
    "                    'assignments': assignment_list[-1],\n",
    "                    'elbo_ests': elbo_ests,\n",
    "                    'steps': n_steps\n",
    "                   }, 'saved_models/{}/mnist_tau_mlp.pth'.format(mlp_type))\n",
    "        \n",
    "    plt.plot(np.array(elbo_ests))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc533y",
   "language": "python",
   "name": "cpsc533y"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
